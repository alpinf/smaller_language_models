{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8E6ZaqcnyIp0"
      },
      "source": [
        "# Vocabulary Trimming of a GPT model\n",
        "\n",
        "This notebook shows how to reduce the size of the vocabulary and of the embedding and decoding layers of a GPT model by removing from the vocabulary the tokens that are not part of a target language.\n",
        "\n",
        "The goal is to transform a multilingual GPT model into a smaller, more efficient monolingual model while preserving the original model's accuracy for the target language. This notebook follows the idea from the paper [Load What You Need](https://aclanthology.org/2020.sustainlp-1.16.pdf). The paper was implemented on BERT models, we will implement it on GPT models.\n",
        "\n",
        "A complete description of this and other methods for reducing model size is available [here](https://github.com/alpinf/smaller_language_models/blob/main/smaller_language_models.md).\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/alpinf/smaller_language_models/blob/main/notebooks/vocab_trim_mGPT.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baOHZkPYGNmo"
      },
      "source": [
        "## Setup\n",
        "\n",
        "This notebook can be run with a CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "Zqzhwf3UGNmp"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "if 'google.colab' not in sys.modules:\n",
        "    print(\"Warning: The setup was only tested in Google Colab\")\n",
        "\n",
        "!python -m pip install pandas==2.2.2 torch==2.3.1 transformers==4.41.2 tokenizers==0.19.1 datasets==2.20.0 gdown tqdm --no-deps --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "J6uyc0YiyIp2"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import json\n",
        "import os\n",
        "from collections import Counter\n",
        "\n",
        "import gdown\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GPT2Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZBCC8SlGNmq"
      },
      "source": [
        "Define the directories where the downloaded data and the model will be saved"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "J5TSykwVGNmq"
      },
      "outputs": [],
      "source": [
        "data_dir = \"./data\"\n",
        "out_dir = \"./out\"\n",
        "\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "os.makedirs(out_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "iDWi4ckkGNmq"
      },
      "outputs": [],
      "source": [
        "def download_file_from_google_drive(file_id, destination):\n",
        "    \"\"\"\n",
        "    Example usage:\n",
        "    File link on Gdrive: https://drive.google.com/file/d/188r2cctPaqmuXnwer3KBpIHpftnE9tCb/view?usp=drive_link\n",
        "\n",
        "    download_file_from_google_drive(\"188r2cctPaqmuXnwer3KBpIHpftnE9tCb\", \"./data/\")\n",
        "    \"\"\"\n",
        "    try:\n",
        "        url = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
        "        gdown.download(url, output=destination, quiet=False)\n",
        "        print(f\"File downloaded from Google Drive to {destination}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading file: {e}\")\n",
        "        raise e\n",
        "\n",
        "def get_device():\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "    elif torch.backends.mps.is_available():\n",
        "        device = torch.device(\"mps\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "    print(f\"Using device {device}\")\n",
        "    return device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmLz1Z_1GNmq"
      },
      "source": [
        "\n",
        "We run the procedure using [mGPT by *ai-forever*](https://huggingface.co/ai-forever/mGPT) as the original model and French as the target language. The model is a multilingual GPT model that supports 50 languages with the same structure of GPT-2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7TLQtluyIp3",
        "outputId": "577a3a08-74a6-4270-9d17-a44529879606"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device cpu\n"
          ]
        }
      ],
      "source": [
        "device = get_device()\n",
        "\n",
        "original_tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/mGPT\")\n",
        "original_model = AutoModelForCausalLM.from_pretrained(\"ai-forever/mGPT\").to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zmyu-BGQyIp3"
      },
      "source": [
        "We start by loading the `merges.txt` and the `vocab.json` files of the original model.\n",
        "\n",
        "The `merges.txt` file contains a list of pairs of tokens. The tokenizer starts by splitting the input text into characters, then it reads the merge file, from top to bottom, and merges all the pairs of characters that are present in the merge file. It continues this process, merging larger and larger tokens, until no more merges are possible.\n",
        "\n",
        "Now that the input text is tokenized, the `vocab.json` file is used to convert the tokens into integers. The file contains a dictionary with the tokens as keys and the corresponding integer as values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "JRADSuXwyIp3"
      },
      "outputs": [],
      "source": [
        "original_vocab = original_tokenizer.get_vocab()\n",
        "\n",
        "tokenizer_json = original_tokenizer.backend_tokenizer.to_str()\n",
        "tokenizer_data = json.loads(tokenizer_json)\n",
        "\n",
        "original_merges = tokenizer_data['model']['merges']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VR-fvP4jyIp4"
      },
      "source": [
        "## Selecting the vocabulary\n",
        "To select the vocabulary we use the [Leipzig corpora collection](https://wortschatz.uni-leipzig.de/en/download/French), but any corpus in the target language can be used. The data we use is part of the **French _News_** section of 2023 with 1 million sentences.\n",
        "\n",
        "The corpus can be downloaded from the link above. We keep a backup copy on our drive for reproducibility.\n",
        "\n",
        "First, we tokenized the entire corpus and kept track of the most frequent tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "2smgZ0v2GNmr"
      },
      "outputs": [],
      "source": [
        "# Download the dataset manually from the link above or use a copy on our drive\n",
        "data_path = os.path.join(data_dir, \"fra_news_2023_1M-sentences.txt\")\n",
        "if not os.path.exists(data_path):\n",
        "    download_file_from_google_drive(\"171RgrkXuWHfY-4BzHR8JEvYp9-D8yPqA\", data_path)\n",
        "    print(\"Dataset downloaded from drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7I27YVbzyIp4",
        "outputId": "114094c7-c654-4761-f79c-c5992956675a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenizing the corpus: 100%|██████████| 1000/1000 [00:32<00:00, 30.58it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The 1000 most common tokens cover 69% of the dataset\n",
            "The 10000 most common tokens cover 97% of the dataset\n",
            "The 30000 most common tokens cover 100% of the dataset\n",
            "\n",
            "Total number of tokens in the dataset: 31 million\n",
            "Number of unique tokens: 36k\n",
            "The seen tokens cover 36.4% of the mGPT vocabulary.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "df_fr = pd.read_csv(data_path, sep=\"\\t\", header=None, quoting=csv.QUOTE_NONE)\n",
        "df_fr.columns = [\"idx\", \"text\"]\n",
        "\n",
        "cnt = Counter()\n",
        "\n",
        "total_tokens = 0\n",
        "batch_size = 1000\n",
        "\n",
        "for i in tqdm(range(0, len(df_fr), batch_size), desc=\"Tokenizing the corpus\"):\n",
        "    batch_texts = df_fr.text[i : i + batch_size].tolist()\n",
        "    encoded_batch = original_tokenizer.batch_encode_plus(\n",
        "        batch_texts,\n",
        "        add_special_tokens=False,\n",
        "        return_attention_mask=False,\n",
        "    )\n",
        "    for input_ids in encoded_batch[\"input_ids\"]:\n",
        "        cnt.update(input_ids)\n",
        "        total_tokens += len(input_ids)\n",
        "\n",
        "for top in 1_000, 10_000, 30_000:\n",
        "    print(f\"The {top} most common tokens cover {100 * sum(v for k, v in cnt.most_common(top)) / sum(cnt.values()):.0f}% of the dataset\")\n",
        "\n",
        "print(f\"\\nTotal number of tokens in the dataset: {total_tokens/1e6:.0f} million\")\n",
        "print(f\"Number of unique tokens: {len(cnt)/1e3:.0f}k\")\n",
        "print(f\"The seen tokens cover {100 * len(cnt) / original_tokenizer.vocab_size:.1f}% of the mGPT vocabulary.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yJD76mEyIp4"
      },
      "source": [
        "We can decide how many tokens to keep based on what percentage of the corpus we want to cover."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P60-fE4hyIp4",
        "outputId": "95922cf9-658d-4288-cd16-85208fd42786"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The 1000 most common tokens cover 69.4% of the dataset\n",
            "The 5000 most common tokens cover 89.9% of the dataset\n",
            "The 10000 most common tokens cover 96.6% of the dataset\n",
            "The 20000 most common tokens cover 99.5% of the dataset\n",
            "The 25000 most common tokens cover 99.8% of the dataset\n",
            "The 26686 most common tokens cover 99.9% of the dataset\n",
            "\n",
            "We will keep 26686 tokens to cover 99.9% of the dataset\n"
          ]
        }
      ],
      "source": [
        "percentage_to_keep = 0.999\n",
        "\n",
        "cum_sum = 0\n",
        "for i, (k, v) in enumerate(cnt.most_common()):\n",
        "    cum_sum += v\n",
        "    if cum_sum / sum(cnt.values()) > percentage_to_keep:\n",
        "        break\n",
        "    num_tokens = i + 1 # we save the number of tokens to keep\n",
        "\n",
        "for top in 1_000, 5_000, 10_000, 20_000, 25_000, num_tokens:\n",
        "    print(f\"The {top} most common tokens cover {100 * sum(v for k, v in cnt.most_common(top)) / sum(cnt.values()):.1f}% of the dataset\")\n",
        "\n",
        "print(f\"\\nWe will keep {num_tokens} tokens to cover {percentage_to_keep * 100}% of the dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86EA3nhuyIp4",
        "outputId": "067470a2-d88b-4e0d-c728-017a0cf22983"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of merges: 99737\n",
            "Total number of tokens: 100k\n",
            "Examples of merges: ['ĠC ent', 'Ġf a', 'ij d', 'o ung', 'os i', 'ĠJah ren', 'alif orn', 'iz ed', 'end ed', 'ep h']\n"
          ]
        }
      ],
      "source": [
        "print(f\"Total number of merges: {len(original_merges)}\")\n",
        "print(f\"Total number of tokens: {len(original_vocab)/1e3:.0f}k\")\n",
        "print(f\"Examples of merges: {original_merges[5000:5010]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxHZDRsmyIp4"
      },
      "source": [
        "If we count the number of lines in the `merge.txt` file, we notice that there are more tokens in the vocabulary than lines in the merges. If we merge all these lines and keep track of the generated tokens we notice that $256 + 7$ tokens are missing.\n",
        "\n",
        "The first 256 tokens missing are used as hexadecimal characters, these tokens will never be the product of a merge, for this reason they do not appear in the merge file.\n",
        "\n",
        "We also have $6$ special characters and the character `Ġ`. Since in the merge file, a merge is divided by a space (*\"a b\"* indicates that *b* has to be merged to *a*), the tokenizer uses the character `Ġ` to indicate a space.\n",
        "\n",
        "We need to keep all these characters in the new vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K14M16fkyIp5",
        "outputId": "5c8cb4e8-c556-4fe3-d7b3-59ff6885dfa0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of tokens that are not in the merges or part of the hexadecimal tokens: 7\n",
            "\n",
            "There are 6 special tokens:\n",
            "bos_token: <s>\n",
            "eos_token: <|endoftext|>\n",
            "unk_token: <unk>\n",
            "sep_token: </s>\n",
            "pad_token: <pad>\n",
            "mask_token: <mask>\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of tokens that are not in the merges or part of the hexadecimal tokens: {len(original_vocab) - len(original_merges) - 256}\")\n",
        "print(f\"\\nThere are {len(original_tokenizer.special_tokens_map)} special tokens:\")\n",
        "\n",
        "for k, v in original_tokenizer.special_tokens_map.items():\n",
        "    print(f\"{k}: {v}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "LW9LmkGqyIp5"
      },
      "outputs": [],
      "source": [
        "# Select the most common tokens to keep\n",
        "new_tokens = set(range(7 + 256))  # the fist 7 contain the special tokens, the next 256 are the byte-pair encodings\n",
        "\n",
        "for i, (k, v) in enumerate(cnt.most_common(num_tokens)):\n",
        "    if k not in new_tokens:\n",
        "        new_tokens.add(k)\n",
        "kept_ids = sorted(new_tokens)\n",
        "\n",
        "inverted_vocab = {v: k for k, v in original_vocab.items()}\n",
        "new_vocab = {inverted_vocab[i]: i for i in kept_ids}\n",
        "\n",
        "# add the element \"Ġ\" to the new_vocab. This character represents the space. It is not in the vocabulary of the tokenizer but it can be used to create merges\n",
        "new_vocab[\"Ġ\"] = original_vocab[\"Ġ\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qStjAaZwyIp5"
      },
      "source": [
        "## Removing Redundant Merges\n",
        "Since we are working with a BPE tokenizer, we cannot simply remove the tokens we do not need from the merge file.\n",
        "\n",
        "**Example 1: Direct removal**\n",
        "\n",
        "Suppose we want to remove the token 'abc' from the vocabulary.\n",
        "If the merge rule ('ab', 'c') => 'abc' remains, the tokenizer will still produce the token 'abc', that we want to remove.\n",
        "For this reason, we need to remove the merge that produces the unwanted token.\n",
        "\n",
        "**Example 2: Chain removal**\n",
        "\n",
        "Furthermore, if we want to remove the 'abc' token we must also remove:\n",
        "1. All merges that use 'abc' directly, for example ('abc', 'd') => 'abcd'\n",
        "2. If 'abcd' isn't in our vocabulary, we must also remove merges using it, like ('abcd', 'e') => 'abcde'\n",
        "3. And so on recursively until we've removed all dependent merges\n",
        "\n",
        "**Solution**\n",
        "\n",
        "We must therefore remove all merge rules that either:\n",
        "1. Would create a token we want to remove\n",
        "2. Use a token we want to remove\n",
        "3. Would create new tokens (not in vocab) that contain tokens we want to remove\n",
        "\n",
        "The implementation uses two functions:\n",
        "1. `trim_merges`:\n",
        "   - Takes the original merges and both vocabularies\n",
        "   - Finds tokens that were removed (in original_vocab but not in new_vocab)\n",
        "   - Processes tokens from shortest to longest to handle substrings properly\n",
        "\n",
        "2. `remove_token_and_dependents`:\n",
        "   - Takes a list of merges, the new vocab, and a token to remove\n",
        "   - First finds all tokens that need to be removed (including dependent tokens)\n",
        "   - Then removes any merges that would create or use these tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "a7EoL3iZyIp5"
      },
      "outputs": [],
      "source": [
        "from typing import List, Set, Tuple, Dict\n",
        "from tqdm import tqdm\n",
        "\n",
        "def remove_token_and_dependents(\n",
        "    merges_list: List[Tuple[str, str]],\n",
        "    vocab: Set[str],\n",
        "    tokens_to_remove: Set[str],\n",
        "    merge_results: Dict[Tuple[str, str], str],\n",
        "    token_to_merges: Dict[str, List[Tuple[str, str]]]\n",
        ") -> List[Tuple[str, str]]:\n",
        "    \"\"\"Remove merges that would create or use given tokens and their dependents\"\"\"\n",
        "    # Find all tokens to remove using BFS instead of recursion\n",
        "    to_process = set([t for t in tokens_to_remove if t not in vocab])\n",
        "    found_tokens = tokens_to_remove.copy()\n",
        "\n",
        "    with tqdm(total=len(to_process), desc=\"Processing tokens\") as pbar:\n",
        "        while to_process:\n",
        "            current_token = to_process.pop()\n",
        "            # Find merges that involve this token\n",
        "            for merge in token_to_merges.get(current_token, []):\n",
        "                result = merge_results[merge]\n",
        "                if result not in vocab and result not in found_tokens:\n",
        "                    found_tokens.add(result)\n",
        "                    to_process.add(result)\n",
        "            pbar.update(1)\n",
        "\n",
        "    # Single pass filter instead of multiple conditions\n",
        "    return [merge for merge in tqdm(merges_list, desc=\"Filtering merges\")\n",
        "            if merge_results[merge] not in found_tokens\n",
        "            and merge[0] not in found_tokens\n",
        "            and merge[1] not in found_tokens]\n",
        "\n",
        "def trim_merges(\n",
        "    original_merges: List[str],\n",
        "    original_vocab: Set[str],\n",
        "    new_vocab: Set[str]\n",
        ") -> List[Tuple[str, str]]:\n",
        "    \"\"\"Remove all merges that could create or use tokens removed from vocabulary.\"\"\"\n",
        "    # Pre-compute all data structures once\n",
        "    merges = [tuple(merge.split()) for merge in tqdm(original_merges, desc=\"Parsing merges\")]\n",
        "\n",
        "    # Sort tokens to remove by length (longest first) to handle nested tokens correctly\n",
        "    tokens_to_remove = sorted(original_vocab - set(new_vocab), key=len, reverse=True)\n",
        "\n",
        "    # Build lookup tables once\n",
        "    merge_results = {merge: merge[0] + merge[1] for merge in tqdm(merges, desc=\"Building merge results\")}\n",
        "    token_to_merges: Dict[str, List[Tuple[str, str]]] = {}\n",
        "\n",
        "    # Single pass to build token_to_merges\n",
        "    for merge in tqdm(merges, desc=\"Building token index\"):\n",
        "        result = merge_results[merge]\n",
        "        for token in (result, merge[0], merge[1]):\n",
        "            if token in token_to_merges:\n",
        "                token_to_merges[token].append(merge)\n",
        "            else:\n",
        "                token_to_merges[token] = [merge]\n",
        "\n",
        "    return remove_token_and_dependents(\n",
        "        merges,\n",
        "        new_vocab,\n",
        "        set(tokens_to_remove),  # Convert sorted list back to set for efficient lookups\n",
        "        merge_results,\n",
        "        token_to_merges\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpxYZeU2vO-_",
        "outputId": "e425d901-e127-42be-89ff-2c1708bd0b56"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Parsing merges: 100%|██████████| 99737/99737 [00:00<00:00, 1625670.54it/s]\n",
            "Building merge results: 100%|██████████| 99737/99737 [00:00<00:00, 2160067.43it/s]\n",
            "Building token index: 100%|██████████| 99737/99737 [00:00<00:00, 210483.17it/s]\n",
            "Processing tokens: 100%|██████████| 73178/73178 [00:00<00:00, 293581.90it/s]\n",
            "Filtering merges: 100%|██████████| 99737/99737 [00:00<00:00, 1411117.85it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Merges reduced from 99737 to 25749\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "updated_merges = trim_merges(\n",
        "    original_merges=original_merges,\n",
        "    original_vocab=set(original_vocab),\n",
        "    new_vocab=new_vocab\n",
        ")\n",
        "print(f\"\\nMerges reduced from {len(original_merges)} to {len(updated_merges)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeGIC6nJyIp6"
      },
      "source": [
        "Finally we can save the new `vocab.json` file and the new `merges.txt` file and create the new tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "T52UyPIHyIp6"
      },
      "outputs": [],
      "source": [
        "with open(f\"{out_dir}/new_vocab.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(new_vocab, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "with open(f\"{out_dir}/new_merges.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for pair in updated_merges:\n",
        "      f.write(f\"{pair[0]} {pair[1]}\\n\")\n",
        "\n",
        "new_tokenizer = GPT2Tokenizer(\n",
        "    f\"{out_dir}/new_vocab.json\",\n",
        "    f\"{out_dir}/new_merges.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLuN8cVTyIp6"
      },
      "source": [
        "## Compare original and new tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxZYmWYYyIp6",
        "outputId": "f08ecd9d-e178-44d9-bc69-7a49e65dfc68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens using the original tokenizer:          ['This', 'Ġan', 'ĠEnglish', 'Ġexample']\n",
            "Tokens using the new tokenizer:               ['Th', 'is', 'Ġan', 'ĠEng', 'l', 'ish', 'Ġex', 'amp', 'le']\n",
            "\n",
            "Decoded text using the original tokenizer:    This an English example\n",
            "Decoded text using the new tokenizer:         This an English example\n",
            "\n",
            "Tokens using the original tokenizer:          ['C', 'eci', 'Ġest', 'Ġun', 'Ġexemple', 'Ġen', 'ĠfranÃ§ais']\n",
            "Tokens using the new tokenizer:               ['C', 'eci', 'Ġest', 'Ġun', 'Ġexemple', 'Ġen', 'ĠfranÃ§ais']\n",
            "\n",
            "Decoded text using the original tokenizer:    Ceci est un exemple en français\n",
            "Decoded text using the new tokenizer:         Ceci est un exemple en français\n"
          ]
        }
      ],
      "source": [
        "english_text = \"This an English example\"\n",
        "\n",
        "original_tokens = original_tokenizer.tokenize(english_text)\n",
        "new_tokens = new_tokenizer.tokenize(english_text)\n",
        "\n",
        "print(f\"{'Tokens using the original tokenizer:'.ljust(45)} {original_tokens}\")\n",
        "print(f\"{'Tokens using the new tokenizer:'.ljust(45)} {new_tokens}\")\n",
        "print()\n",
        "\n",
        "input_ids_original = original_tokenizer.convert_tokens_to_ids(original_tokens)\n",
        "input_ids_new = new_tokenizer.convert_tokens_to_ids(new_tokens)\n",
        "\n",
        "original_decoded_output = original_tokenizer.decode(input_ids_original)\n",
        "new_decoded_output = new_tokenizer.decode(input_ids_new)\n",
        "print(f\"{'Decoded text using the original tokenizer:'.ljust(45)} {original_decoded_output}\")\n",
        "print(f\"{'Decoded text using the new tokenizer:'.ljust(45)} {new_decoded_output}\")\n",
        "print()\n",
        "\n",
        "french_text = \"Ceci est un exemple en français\"\n",
        "\n",
        "original_tokens = original_tokenizer.tokenize(french_text)\n",
        "new_tokens = new_tokenizer.tokenize(french_text)\n",
        "\n",
        "print(f\"{'Tokens using the original tokenizer:'.ljust(45)} {original_tokens}\")\n",
        "print(f\"{'Tokens using the new tokenizer:'.ljust(45)} {new_tokens}\")\n",
        "print()\n",
        "\n",
        "input_ids_original = original_tokenizer.convert_tokens_to_ids(original_tokens)\n",
        "input_ids_new = new_tokenizer.convert_tokens_to_ids(new_tokens)\n",
        "\n",
        "original_decoded_output = original_tokenizer.decode(input_ids_original)\n",
        "new_decoded_output = new_tokenizer.decode(input_ids_new)\n",
        "print(f\"{'Decoded text using the original tokenizer:'.ljust(45)} {original_decoded_output}\")\n",
        "print(f\"{'Decoded text using the new tokenizer:'.ljust(45)} {new_decoded_output}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZeggzHVGNmt"
      },
      "source": [
        "- We can see that while embedding the **English sentence**, there are some differences between the two tokenizers. This is because not all of the *English* tokens have been retained.\n",
        "- On the other hand, when we encode the **French sentence**, the embeddings are the same. This is because all of the *French* tokens have been retained. The tokens have been remapped to the new vocabulary, so the IDs are different, but the embeddings are the same.\n",
        "\n",
        "We can also notice that the decoding works correctly, both for the *English* and *French* tokens.\n",
        "\n",
        "When we talk about *French* or *English* tokens, we refer to the tokens that are used to tokenize most of the sentences in the respective languages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_b6Ai_h0yIp6"
      },
      "source": [
        "## Updating the embedding layer\n",
        "To reduce the size of the original model we need to modify the embedding layer and the head layer of the original model. The head layer is the last layer of the model, that is used to predict the next token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "7XVjwSJJyIp6"
      },
      "outputs": [],
      "source": [
        "new_model = AutoModelForCausalLM.from_pretrained(\"ai-forever/mGPT\").to(device)\n",
        "\n",
        "new_size = len(kept_ids)\n",
        "\n",
        "embedding_dim = original_model.transformer.wte.weight.data.shape[1]\n",
        "\n",
        "new_emb = torch.nn.Embedding(new_size, embedding_dim)\n",
        "new_head = torch.nn.Linear(embedding_dim, new_size)\n",
        "\n",
        "for new_id, old_id in enumerate(kept_ids):\n",
        "    # copy the original weights of the embeddings and the head\n",
        "    new_emb.weight.data[new_id] = original_model.transformer.wte.weight.data[old_id]\n",
        "    new_head.weight.data[new_id] = original_model.lm_head.weight.data[old_id]\n",
        "\n",
        "# add the new embeddings and head to the model\n",
        "new_model.transformer.wte = new_emb\n",
        "new_model.lm_head = new_head\n",
        "\n",
        "#  update the model configuration\n",
        "new_model.config.__dict__['vocab_size'] = new_size\n",
        "new_model.config.__dict__['_name_or_path'] = f\"{out_dir}/frGPT\"\n",
        "\n",
        "new_vocab_final = {}\n",
        "for new_token, old_token in enumerate(kept_ids):\n",
        "    new_vocab_final[inverted_vocab[old_token]] = new_token\n",
        "\n",
        "with open(f\"{out_dir}/new_vocab.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(new_vocab_final, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# load the new tokenizer\n",
        "new_tokenizer = GPT2Tokenizer(\n",
        "    f\"{out_dir}/new_vocab.json\",\n",
        "    f\"{out_dir}/new_merges.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7dlbfZeGNmt"
      },
      "source": [
        "We can save the new model and use it for inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mB4qSK1RGNmu",
        "outputId": "baafc8ff-8fe2-4a0c-b9a5-31aa3382cd0e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('./out/frGPT/tokenizer_config.json',\n",
              " './out/frGPT/special_tokens_map.json',\n",
              " './out/frGPT/vocab.json',\n",
              " './out/frGPT/merges.txt',\n",
              " './out/frGPT/added_tokens.json')"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# save the new model and the tokenizer\n",
        "new_model.save_pretrained(f\"{out_dir}/frGPT\")\n",
        "new_tokenizer.save_pretrained(f\"{out_dir}/frGPT\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cHgEkp_GNmu"
      },
      "source": [
        "Finally we can use the new model to generate text in the target language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DzuJQ6pyIp6",
        "outputId": "79334e6e-c73e-4836-c8ba-6b96e82ac825"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Il était une fois, dans un pays lointain, un étrange pays, dans un monde éloigné, à l’autre bout de la terre, avec des rois gouvernés par des étrangers. Ils étaient tous assez grands pour nous faire la guerre de clan, nous faire massacrer, se battre contre nous. Mais ils étaient tous beaux, et ils étaient tous loin de nous. Ils ne sont pas assez grands pour être propres, ils ne sont pas intelligents, et ils sont tout sauf\n"
          ]
        }
      ],
      "source": [
        "text = \"Il était une fois, dans un pays lointain\"\n",
        "\n",
        "new_model.to(device)\n",
        "\n",
        "input_ids = new_tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
        "out = new_model.generate(\n",
        "        input_ids,\n",
        "        min_length=100,\n",
        "        max_length=100,\n",
        "        eos_token_id=5,\n",
        "        top_k=0,\n",
        "        top_p=0.95,\n",
        "        no_repeat_ngram_size=4,\n",
        "        do_sample=True\n",
        ")\n",
        "generated_text = list(map(new_tokenizer.decode, out))[0]\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWwr2kvcGNmu"
      },
      "source": [
        "When we print the structure of the model we can see that the new model has a smaller embedding layer and a smaller head layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZyjrFp7yIp7",
        "outputId": "8e98b194-93ec-4955-d437-f277da377cad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT2LMHeadModel(\n",
            "  (transformer): GPT2Model(\n",
            "    (wte): Embedding(26822, 2048)\n",
            "    (wpe): Embedding(2048, 2048)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-23): 24 x GPT2Block(\n",
            "        (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=2048, out_features=26822, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(new_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-ZoazSdGNmu"
      },
      "source": [
        "To understand how much we have reduced the size of the model we can compare the number of parameters of the original model with the number of parameters of the new model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpVsj90YGNmz",
        "outputId": "edc5fa8b-0901-48bc-87c9-c9748c30eeff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of parameters in the new model is 1323 million\n",
            "Number of parameters in the original model is 1418 million\n",
            "The percentage of parameters that are in the embedding layer is 4.2%\n",
            "Percentage of parameters in the head layer: 4.2%\n",
            "Percentage of tokens kept in the new model: 26.8%\n",
            "Percentage of parameters of the new model in comparison to the original model: 93.3%\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of parameters in the new model is {new_model.num_parameters()/ 1e6:.0f} million\")\n",
        "print(f\"Number of parameters in the original model is {original_model.num_parameters()/ 1e6:.0f} million\")\n",
        "\n",
        "print(f\"The percentage of parameters that are in the embedding layer is {100 *  new_model.transformer.wte.weight.data.numel() / new_model.num_parameters():.1f}%\")\n",
        "print(f\"Percentage of parameters in the head layer: {100 * new_model.lm_head.weight.data.numel() / new_model.num_parameters():.1f}%\")\n",
        "print(f\"Percentage of tokens kept in the new model: {100 * len(kept_ids) / original_tokenizer.vocab_size:.1f}%\")\n",
        "print(f\"Percentage of parameters of the new model in comparison to the original model: {100 * new_model.num_parameters() / original_model.num_parameters():.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDvpB7wcGNmz"
      },
      "source": [
        "## Conclusion\n",
        "We have successfully reduced the size of the vocabulary of the tokenizer, keeping only 27% of the initial 100k tokens. We have also reduced the size of the embedding layer and the head layer of the model by 74% each, with an overall reduction in the number of parameters of about 7%.\n",
        "\n",
        "The reduction in size is not very significant, so this is not necessarily the preferred approach for decoder-only models if the main objective is size reduction. It may still be a good starting point to further reduce the model's size, while using a more focused vocabulary."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
